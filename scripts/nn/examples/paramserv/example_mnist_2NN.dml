#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------

/*
 * This script aims to provide an entry point to test a simple feed forward neural network
 * on the mnist digit recognition dataset in the different execution schemes.
 * (Single threaded, Parameter Server or Federated)
 * It also provides an example of how a feed forward neural network is implemented in DML
 * for the different execution schemes.
 *
 * Nvargs to specify when executing via the CLI:
 *  Required:
 *   - train: ABSOLUTE path to the trainings data
 *   - test: ABSOLUTE path to the test data
 *   - C: Number of channels in the input data
 *   - Hin: Input data height
 *   - Win: Input data width
 *  Optional and General:
 *   - epochs: Number of epochs to train on
 *   - batch_size: Batch size
 *   - learning_rate: Learning rate "eta"
 *   - mode: Whether to run single threaded or via the parameter server
 *  Optional and only needed for the Parameter Server
 *   - workers: Number of workers to spawn
 *   - workers: Number of workers to create
 *   - utype: parameter server framework to use
 *   - scheme: update schema
 *   - paramserv_mode: local or distributed
 *
 * Outputs:
 *  Writes all information to stdout
 *
 * The MNIST Data can be downloaded as follows:
 *  mkdir -p data/mnist/
 *  cd data/mnist/
 *  curl -O https://pjreddie.com/media/files/mnist_train.csv
 *  curl -O https://pjreddie.com/media/files/mnist_test.csv
 *
 * Sample Invocation
 *  - Single Threaded:
 *      systemds ~/Developer/systemds/scripts/nn/examples/paramserv/example_mnist_2NN.dml -nvargs mode=NORMAL epochs=2 batch_size=32 learning_rate=0.01 train=/Users/tobias/Developer/mnist_data/mnist_train.csv test=/Users/tobias/Developer/mnist_data/mnist_test.csv
 *  - With the Parameter Server
 *      systemds ~/Developer/systemds/scripts/nn/examples/paramserv/example_mnist_2NN.dml -nvargs mode=PARAMSERV epochs=8 batch_size=32 workers=10 learning_rate=0.001 train=/Users/tobias/Developer/mnist_data/mnist_train.csv test=/Users/tobias/Developer/mnist_data/mnist_test.csv
 */

source("scripts/nn/examples/paramserv/TwoNN.dml") as TwoNN
source("scripts/nn/layers/cross_entropy_loss.dml") as cross_entropy_loss

# Parse nvargs
## general args
train_data = $train
test_data = $test
epochs = ifdef($epochs, 1)
batch_size = ifdef($batch_size, 32)
learning_rate = ifdef($learning_rate, 0.01)
mode = ifdef($mode, "NORMAL")
## paramserv specific args
workers = ifdef($workers, 1)
utype = ifdef($utype, "BSP")
freq = ifdef($freq, "BATCH")
scheme = ifdef($scheme, "DISJOINT_CONTIGUOUS")
paramserv_mode = ifdef($paramserv_mode, "LOCAL")

# Read training data
data = read(train_data, format="csv")
n = nrow(data)
# Extract images and labels
images = data[,2:ncol(data)]
labels = data[,1]
# Scale images to [0,1], and one-hot encode the labels
images = images / 255.0
labels = table(seq(1, n), labels+1, n, 10)
# Split into training (55,000 examples) and validation (5,000 examples)
X = images[5001:nrow(images),]
X_val = images[1:5000,]
y = labels[5001:nrow(images),]
y_val = labels[1:5000,]

# differentiate test modes
if (mode == "NORMAL") {
  model = TwoNN::train(X, y, X_val, y_val, epochs, batch_size, learning_rate)
} else if (mode == "PARAMSERV") {
  model = TwoNN::train_paramserv(X, y, X_val, y_val, epochs, workers, utype, freq, batch_size, scheme, paramserv_mode, learning_rate)
}

# Compute validation loss & accuracy
probs_val = TwoNN::predict(X_val, model)
[loss_val, accuracy_val] = TwoNN::eval(probs_val, y_val)
print("\n[+] validation loss: " + loss_val + ", validation accuracy: " + accuracy_val)

# Read test data
data = read(test_data, format="csv")
n = nrow(data)
# Extract images and labels
X_test = data[,2:ncol(data)]
y_test = data[,1]
# Scale images to [0,1], and one-hot encode the labels
X_test = X_test / 255.0
y_test = table(seq(1, n), y_test+1, n, 10)

# Compute test loss & accuracy
probs_test = TwoNN::predict(X_test, model)
[loss_test, accuracy_test] = TwoNN::eval(probs_test, y_test)
print("[+] test loss: " + loss_test + ", test accuracy: " + accuracy_test + "\n")